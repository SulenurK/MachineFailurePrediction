# -*- coding: utf-8 -*-
"""ml_functions_binary.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VmZmDZjKyPDJ2PysxhwPJg74QvC2eE6x
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                            f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report)
from sklearn.model_selection import cross_val_score, StratifiedKFold , GridSearchCV, RandomizedSearchCV
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier)
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

"""#ML"""

def cros_val(model, X, y, cv=5):
  # Cross-validated metrics
  cv_accuracy = cross_val_score(model, X, y, cv=StratifiedKFold(n_splits=cv), scoring='accuracy').mean()
  cv_precision = cross_val_score(model, X, y, cv=StratifiedKFold(n_splits=cv), scoring='precision').mean()
  cv_recall = cross_val_score(model, X, y, cv=StratifiedKFold(n_splits=cv), scoring='recall').mean()
  cv_f1 = cross_val_score(model, X, y, cv=StratifiedKFold(n_splits=cv), scoring='f1').mean()

  print("\nCross-validated Metrics:")
  print(f"Accuracy: {cv_accuracy:.4f}")
  print(f"Precision: {cv_precision:.4f}")
  print(f"Recall: {cv_recall:.4f}")
  print(f"F1 Score: {cv_f1:.4f}")

"""##Binary Classification"""

# Model training functions
def learn_model_logreg(X_train, y_train, params=None):
    """Train Logistic Regression model"""
    model = LogisticRegression(max_iter=1000)
    if params:
        model.set_params(**params)
    # Cross-validated metrics
    cros_val(model, X_train, y_train)

    model.fit(X_train, y_train)
    return model

def learn_model_rf(X_train, y_train, params=None):
    """Train Random Forest model"""
    model = RandomForestClassifier()
    if params:
        model.set_params(**params)
    # Cross-validated metrics
    cros_val(model, X_train, y_train)

    model.fit(X_train, y_train)
    return model

def learn_model_gb(X_train, y_train, params=None):
    """Train Gradient Boosting model"""
    model = GradientBoostingClassifier()
    if params:
        model.set_params(**params)
    # Cross-validated metrics
    cros_val(model, X_train, y_train)

    model.fit(X_train, y_train)
    return model

def learn_model_svm(X_train, y_train, params=None):
    """Train SVM model"""
    model = SVC(probability=True)
    if params:
        model.set_params(**params)
    # Cross-validated metrics
    cros_val(model, X_train, y_train)

    model.fit(X_train, y_train)
    return model

def learn_model_knn(X_train, y_train, params=None):
    """Train KNN model"""
    model = KNeighborsClassifier()
    if params:
        model.set_params(**params)
    # Cross-validated metrics
    cros_val(model, X_train, y_train)

    model.fit(X_train, y_train)
    return model

def learn_model_dt(X_train, y_train, params=None):
    """Train Decision Tree model"""
    model = DecisionTreeClassifier()
    if params:
        model.set_params(**params)
    # Cross-validated metrics
    cros_val(model, X_train, y_train)

    model.fit(X_train, y_train)
    return model

def learn_model_nb(X_train, y_train, params=None):
    """Train Naive Bayes model"""
    model = GaussianNB()
    if params:
        model.set_params(**params)
    # Cross-validated metrics
    cros_val(model, X_train, y_train)

    model.fit(X_train, y_train)
    return model

def learn_model_xgb(X_train, y_train, params=None):
    """Train XGBoost model"""
    model = XGBClassifier()
    if params:
        model.set_params(**params)
    # Cross-validated metrics
    cros_val(model, X_train, y_train)

    model.fit(X_train, y_train)
    return model

def learn_model_lgbm(X_train, y_train, params=None):
    """Train LightGBM model"""
    model = LGBMClassifier()
    if params:
        model.set_params(**params)
    # Cross-validated metrics
    cros_val(model, X_train, y_train)

    model.fit(X_train, y_train)
    return model

def apply_model(X, y, model):
    """
    Apply model to data, calculate metrics and return predictions

    Parameters:
    X (DataFrame/array): Feature data
    y (Series/array): Target values
    model: Initialized model object
    params (dict): Model parameters (optional)
    cv (int): Number of cross-validation folds (default: 5)
    print_metrics (bool): Whether to print evaluation metrics (default: True)

    Returns:
    DataFrame: Predictions with probabilities (if available)
    """
    # Train model
    model.fit(X, y)
    predictions = model.predict(X)

    # Create results DataFrame
    results = pd.DataFrame({'predictions': predictions}, index=pd.DataFrame(X).index)

    if hasattr(model, "predict_proba"):
        results['probabilities'] = model.predict_proba(X)[:, 1]

    return results

def evaluate_model(df_pred, y_true):
    y_pred = df_pred['predictions'] if 'predictions' in df_pred else df_pred

    print("---Classification Report---")
    print(classification_report(y_true, y_pred))

    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.show()

def fine_tune_binary_classifier(model_name, X_train, y_train, cv=5, scoring='precision', random_state=42):
    """
    Fine-tunes a binary classification model using GridSearchCV or RandomizedSearchCV.
    If the model supports feature importance, it also displays a feature importance graph and scores.

    Parameters:
    model_name (str): Name of the model to fine-tune
    X_train (array-like): Training data (features)
    y_train (array-like): Target variable (binary labels)
    cv (int): Number of cross-validation folds (default: 5)
    scoring (str): Scoring metric for GridSearchCV (default: 'precision')
    random_state (int): Random seed for reproducibility (default: 42)

    Returns:
    best_params (dict): Best hyperparameters found
    """

    # Define the model and parameter grid
    if model_name == 'Logistic Regression':
        model = LogisticRegression(max_iter=1000, random_state=random_state)
        param_grid = {
            'C': [0.01, 0.1, 1, 10, 100],
            'penalty': ['l1', 'l2'],
            'solver': ['liblinear']
        }
    elif model_name == 'Random Forest':
        model = RandomForestClassifier(random_state=random_state)
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [None, 10, 20, 30],
            'min_samples_split': [2, 5, 10]
        }
    elif model_name == 'Gradient Boosting':
        model = GradientBoostingClassifier(random_state=random_state)
        param_grid = {
            'n_estimators': [50, 100, 200],
            'learning_rate': [0.01, 0.1, 0.2],
            'max_depth': [3, 5, 7]
        }
    elif model_name == 'SVM':
        model = SVC(probability=True, random_state=random_state)
        param_grid = {
            'C': [0.1, 1, 10],
            'kernel': ['linear', 'rbf'],
            'gamma': ['scale', 'auto']
        }
    elif model_name == 'K-Nearest Neighbors':
        model = KNeighborsClassifier()
        param_grid = {
            'n_neighbors': [3, 5, 7, 9],
            'weights': ['uniform', 'distance']
        }
    elif model_name == 'Decision Tree':
        model = DecisionTreeClassifier(random_state=random_state)
        param_grid = {
            'max_depth': [None, 10, 20, 30],
            'min_samples_split': [2, 5, 10]
        }
    elif model_name == 'Naive Bayes':
        model = GaussianNB()
        param_grid = {}  # No hyperparameters to tune
    elif model_name == 'XGBoost':
        model = XGBClassifier(random_state=random_state, eval_metric='logloss')
        param_grid = {
            'n_estimators': [50, 100, 200],
            'learning_rate': [0.01, 0.1, 0.2],
            'max_depth': [3, 5, 7]
        }
    elif model_name == 'LightGBM':
        model = LGBMClassifier(random_state=random_state)
        param_grid = {
            'n_estimators': [50, 100, 200],
            'learning_rate': [0.01, 0.1, 0.2],
            'max_depth': [3, 5, 7]
        }
    else:
        raise ValueError(f"Unknown model name: {model_name}")

    # Perform grid search
    if param_grid:
        grid_search = GridSearchCV(model, param_grid, cv=cv, scoring=scoring, n_jobs=-1)
        grid_search.fit(X_train, y_train)
        best_params = grid_search.best_params_
        best_model = grid_search.best_estimator_
    else:
        best_params = {}
        best_model = model.fit(X_train, y_train)

    # Feature Importance for supported models
    if hasattr(best_model, 'feature_importances_'):
        # Get feature importances
        feature_importances = best_model.feature_importances_
        feature_names = X_train.columns if hasattr(X_train, 'columns') else range(X_train.shape[1])

        # Create a DataFrame for feature importances
        importance_df = pd.DataFrame({
            'Feature': feature_names,
            'Importance': feature_importances
        }).sort_values(by='Importance', ascending=False)

        # Plot feature importance
        plt.figure(figsize=(10, 6))
        sns.barplot(x='Importance', y='Feature', data=importance_df)
        plt.title(f'{model_name} Feature Importance')
        plt.show()

        # Print feature importance scores
        print("Feature Importance Scores:")
        print(importance_df)

    elif hasattr(best_model, 'coef_'):
        # For linear models like Logistic Regression
        coefficients = best_model.coef_[0]
        feature_names = X_train.columns if hasattr(X_train, 'columns') else range(X_train.shape[1])

        # Create a DataFrame for coefficients
        coef_df = pd.DataFrame({
            'Feature': feature_names,
            'Coefficient': coefficients
        }).sort_values(by='Coefficient', ascending=False)

        # Plot coefficients
        plt.figure(figsize=(10, 6))
        sns.barplot(x='Coefficient', y='Feature', data=coef_df)
        plt.title(f'{model_name} Feature Coefficients')
        plt.show()

        # Print coefficients
        print("Feature Coefficients:")
        print(coef_df)

    elif model_name == 'XGBoost':
        # For XGBoost, use get_score() for feature importance
        feature_importances = best_model.get_booster().get_score(importance_type='weight')
        feature_names = list(feature_importances.keys())
        importance_values = list(feature_importances.values())

        # Create a DataFrame for feature importances
        importance_df = pd.DataFrame({
            'Feature': feature_names,
            'Importance': importance_values
        }).sort_values(by='Importance', ascending=False)

        # Plot feature importance
        plt.figure(figsize=(10, 6))
        sns.barplot(x='Importance', y='Feature', data=importance_df)
        plt.title(f'{model_name} Feature Importance')
        plt.show()

        # Print feature importance scores
        print("Feature Importance Scores:")
        print(importance_df)

    else:
        print(f"Feature importance is not supported for {model_name}.")

    return best_params